{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSR6_2eYRNGZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "base_path = \"/content/drive/MyDrive\"\n",
    "dataset_dir = os.path.join(base_path, \"dataset furniture\")\n",
    "\n",
    "filenames = os.listdir(dataset_dir)\n",
    "print(f\"ðŸ“ Total files in folder: {len(filenames)}\")"
   ],
   "metadata": {
    "id": "i1YLX-60RPFL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "drawn_ids = set()\n",
    "original_ids = set()\n",
    "\n",
    "for fname in filenames:\n",
    "    if \"_drawn\" in fname:\n",
    "        image_id = fname.replace(\"fimage_\", \"\").split(\"_drawn\")[0]\n",
    "        drawn_ids.add(image_id)\n",
    "\n",
    "    elif \"_original\" in fname:\n",
    "        image_id = fname.replace(\"fimage_\", \"\").split(\"_original\")[0]\n",
    "        original_ids.add(image_id)\n",
    "\n",
    "paired_ids = drawn_ids & original_ids"
   ],
   "metadata": {
    "id": "m1W9yH4nRQis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"ðŸ–Œï¸ Total sketch images: {len(drawn_ids)}\")\n",
    "print(f\"ðŸ–¼ï¸ Total original images: {len(original_ids)}\")\n",
    "print(f\"âœ… Found {len(paired_ids)} sketch-original image pairs.\")"
   ],
   "metadata": {
    "id": "N9KaQaydRRzs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train Test split"
   ],
   "metadata": {
    "id": "3RhRzCPtRT-i"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 1: Index all files by ID and type (drawn / original)\n",
    "file_mapping = {}\n",
    "\n",
    "for fname in filenames:\n",
    "    if not fname.startswith(\"fimage\"):\n",
    "        continue\n",
    "\n",
    "    if \"_drawn\" in fname:\n",
    "        image_id = fname.replace(\"fimage\", \"\").split(\"_drawn\")[0]\n",
    "        file_mapping.setdefault(image_id, {})['drawn'] = fname\n",
    "    elif \"_original\" in fname:\n",
    "        image_id = fname.replace(\"fimage\", \"\").split(\"_original\")[0]\n",
    "        file_mapping.setdefault(image_id, {})['original'] = fname\n",
    "\n",
    "# Step 2: Filter to only include complete pairs\n",
    "paired_ids = sorted([img_id for img_id, pair in file_mapping.items() if 'drawn' in pair and 'original' in pair])\n",
    "\n",
    "# Step 3: Train/test split\n",
    "split_idx = int(0.7 * len(paired_ids))\n",
    "train_ids = paired_ids[:split_idx]\n",
    "test_ids = paired_ids[split_idx:]\n",
    "\n",
    "# Step 4: Build train and test filenames\n",
    "train_filenames = []\n",
    "test_filenames = []\n",
    "\n",
    "for image_id in train_ids:\n",
    "    pair = file_mapping[image_id]\n",
    "    train_filenames.append(pair['drawn'])\n",
    "    train_filenames.append(pair['original'])\n",
    "\n",
    "for image_id in test_ids:\n",
    "    pair = file_mapping[image_id]\n",
    "    test_filenames.append(pair['drawn'])\n",
    "    test_filenames.append(pair['original'])\n",
    "\n",
    "print(f\"ðŸ›¤ï¸ Train set size: {len(train_filenames)} images\")\n",
    "print(f\"ðŸ§ª Test set size: {len(test_filenames)} images\")"
   ],
   "metadata": {
    "id": "Sus5NtKyRWkX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define source and destination directories\n",
    "train_dir = os.path.join(base_path, \"dataset furniture\", \"train\")\n",
    "test_dir = os.path.join(base_path, \"dataset furniture\", \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Function to copy files if not already present\n",
    "def copy_files(file_list, src_dir, dst_dir):\n",
    "    for fname in file_list:\n",
    "        src_path = os.path.join(src_dir, fname)\n",
    "        dst_path = os.path.join(dst_dir, fname)\n",
    "\n",
    "        # Only copy if the file doesn't exist in the destination\n",
    "        if not os.path.exists(dst_path):\n",
    "            shutil.copyfile(src_path, dst_path)\n",
    "            print(f\"Copied: {fname}\")  # You can remove this line if not needed for debugging\n",
    "        else:\n",
    "            print(f\"Skipped (already exists): {fname}\")  # Optional, for feedback\n",
    "\n",
    "# Copy train files only if they don't exist\n",
    "copy_files(train_filenames, dataset_dir, train_dir)\n",
    "\n",
    "# Copy test files only if they don't exist\n",
    "copy_files(test_filenames, dataset_dir, test_dir)\n",
    "\n",
    "print(\"âœ… Files successfully copied to train/ and test/ folders.\")"
   ],
   "metadata": {
    "id": "gANAeoIFRYFc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pre Processing"
   ],
   "metadata": {
    "id": "OSd_3CsEYQMD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Resizing"
   ],
   "metadata": {
    "id": "pJWkssshYUBH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Function to get the size of an image\n",
    "def get_image_size(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        return img.size  # Returns (width, height)\n",
    "\n",
    "# Checking the size of a sample image in the dataset\n",
    "sample_image_path = os.path.join(dataset_dir, train_filenames[0])  # Use any file in the dataset\n",
    "image_size = get_image_size(sample_image_path)\n",
    "print(f\"Sample image size: {image_size}\")"
   ],
   "metadata": {
    "id": "hZemWQHjRZxe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Normalization"
   ],
   "metadata": {
    "id": "KSIgEHw5YXbL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Create a folder to save normalized images\n",
    "normalized_train_dir = os.path.join(dataset_dir, \"normalized_train\")\n",
    "normalized_test_dir = os.path.join(dataset_dir, \"normalized_test\")\n",
    "\n",
    "os.makedirs(normalized_train_dir, exist_ok=True)\n",
    "os.makedirs(normalized_test_dir, exist_ok=True)\n",
    "\n",
    "# Function to normalize image and save it\n",
    "def normalize_image(image_path, save_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        img_array = np.array(img).astype(np.float32)\n",
    "        img_array = (img_array / 127.5) - 1.0\n",
    "        normalized_img = Image.fromarray(((img_array + 1.0) * 127.5).astype(np.uint8))\n",
    "        normalized_img.save(save_path)\n",
    "        print(f\"Saved normalized image to {save_path}\")\n",
    "\n",
    "# Normalize and save all images in the training and test sets\n",
    "# for fname in train_filenames:\n",
    "#     image_path = os.path.join(train_dir, fname)\n",
    "#     save_path = os.path.join(normalized_train_dir, fname)\n",
    "#     normalize_image(image_path, save_path)\n",
    "\n",
    "# for fname in test_filenames:\n",
    "#     image_path = os.path.join(test_dir, fname)\n",
    "#     save_path = os.path.join(normalized_test_dir, fname)\n",
    "#     normalize_image(image_path, save_path)\n",
    "\n",
    "def check_random_normalized_images(file_list, normalized_dir, num_samples=2):\n",
    "    samples = random.sample(file_list, min(num_samples, len(file_list)))\n",
    "    all_correct = True\n",
    "\n",
    "    for fname in samples:\n",
    "        normalized_path = os.path.join(normalized_dir, fname)\n",
    "        with Image.open(normalized_path) as img:\n",
    "            img_array = np.array(img)\n",
    "            if img_array.max() > 255 or img_array.min() < 0:\n",
    "                print(f\"âŒ Image {fname} has incorrect normalization values!\")\n",
    "                all_correct = False\n",
    "            else:\n",
    "                print(f\"âœ… Image {fname} appears correctly normalized.\")\n",
    "\n",
    "    if all_correct:\n",
    "        print(\"âœ… All images normalized and saved successfully!\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Some images in {normalized_dir} may not be normalized correctly.\")\n",
    "\n",
    "check_random_normalized_images(train_filenames, normalized_train_dir)\n",
    "check_random_normalized_images(test_filenames, normalized_test_dir)"
   ],
   "metadata": {
    "id": "btLlDGCiXhPT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Extract training image IDs (correctly)\n",
    "train_ids = [fname.replace(\"fimage\", \"\").split(\"_drawn\")[0] for fname in train_filenames if \"_drawn\" in fname]\n",
    "\n",
    "# Step 2: Split image IDs into train/val\n",
    "train_ids_split, val_ids_split = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Group filenames again by ID\n",
    "val_filenames = []\n",
    "new_train_filenames = []\n",
    "\n",
    "for image_id in train_ids_split:\n",
    "    if image_id in file_mapping:\n",
    "        pair = file_mapping[image_id]\n",
    "        new_train_filenames.append(pair['drawn'])\n",
    "        new_train_filenames.append(pair['original'])\n",
    "\n",
    "for image_id in val_ids_split:\n",
    "    if image_id in file_mapping:\n",
    "        pair = file_mapping[image_id]\n",
    "        val_filenames.append(pair['drawn'])\n",
    "        val_filenames.append(pair['original'])\n",
    "\n",
    "# âœ… Summary\n",
    "print(f\"ðŸ›¤ï¸ Train set: {len(new_train_filenames)} images\")\n",
    "print(f\"ðŸ§ª Validation set: {len(val_filenames)} images\")\n",
    "print(f\"ðŸ§ª Test set (unchanged): {len(test_filenames)} images\")"
   ],
   "metadata": {
    "id": "8M0KkreGXj_F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define path for validation directory\n",
    "val_dir = os.path.join(dataset_dir, \"normalized_val\")\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Function to copy validation images\n",
    "def copy_files_if_needed(file_list, source_dir, destination_dir, expected_file_count):\n",
    "    # Check if the validation directory already has the expected number of files\n",
    "    existing_files = len([f for f in os.listdir(destination_dir) if os.path.isfile(os.path.join(destination_dir, f))])\n",
    "\n",
    "    if existing_files >= expected_file_count:\n",
    "        print(f\"â© Skipped copying: {existing_files} files already exist in {destination_dir}.\")\n",
    "        return  # Exit if the expected number of files are already present\n",
    "\n",
    "    for fname in file_list:\n",
    "        src_path = os.path.join(source_dir, fname)\n",
    "        dst_path = os.path.join(destination_dir, fname)\n",
    "\n",
    "        if not os.path.exists(src_path):\n",
    "            print(f\"âš ï¸ Warning: Source file not found: {src_path}\")\n",
    "        elif os.path.exists(dst_path):\n",
    "            print(f\"â© Skipped (already exists): {fname}\")\n",
    "        else:\n",
    "            shutil.copy(src_path, dst_path)\n",
    "            print(f\"âœ… Copied: {fname}\")\n",
    "\n",
    "# Expected number of files in validation set\n",
    "expected_val_file_count = 428\n",
    "\n",
    "# Copy validation files\n",
    "copy_files_if_needed(val_filenames, train_dir, val_dir, expected_val_file_count)\n",
    "\n",
    "print(\"âœ… Validation files checked and copied if needed.\")"
   ],
   "metadata": {
    "id": "pnXSuVxYXmHo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Preparation for training"
   ],
   "metadata": {
    "id": "1PNj7vJYYiOC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Function to count the files in the subfolders\n",
    "def count_files_in_subfolders(base_dir):\n",
    "    original_dir = os.path.join(dataset_dir, \"original\")\n",
    "    drawn_dir = os.path.join(dataset_dir, \"drawn\")\n",
    "\n",
    "    num_original_files = len(os.listdir(original_dir)) if os.path.exists(original_dir) else 0\n",
    "    num_drawn_files = len(os.listdir(drawn_dir)) if os.path.exists(drawn_dir) else 0\n",
    "\n",
    "    return num_original_files, num_drawn_files\n",
    "\n",
    "# Function to create subfolders and copy files (if not already copied)\n",
    "def create_subfolders_and_copy_files(base_dir, expected_original_files, expected_drawn_files):\n",
    "    original_dir = os.path.join(base_dir, \"original\")\n",
    "    drawn_dir = os.path.join(base_dir, \"drawn\")\n",
    "\n",
    "    os.makedirs(original_dir, exist_ok=True)\n",
    "    os.makedirs(drawn_dir, exist_ok=True)\n",
    "\n",
    "    num_original_files, num_drawn_files = count_files_in_subfolders(base_dir)\n",
    "\n",
    "    if num_original_files >= expected_original_files and num_drawn_files >= expected_drawn_files:\n",
    "        print(f\"â© Skipping folder: {base_dir}, already contains enough original and drawn images.\")\n",
    "        return\n",
    "\n",
    "    # Loop through all files in the base directory (not subfolders)\n",
    "    for fname in os.listdir(base_dir):\n",
    "        file_path = os.path.join(base_dir, fname)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue  # Skip if not a file\n",
    "\n",
    "        # Process drawn images\n",
    "        if \"_drawn\" in fname:\n",
    "            image_id = fname.replace(\"image_\", \"\").split(\"_drawn\")[0]\n",
    "            original_fname = f\"image_{image_id}_original.png\"\n",
    "\n",
    "            # Copy drawn image if not already copied\n",
    "            dst_drawn_path = os.path.join(drawn_dir, fname)\n",
    "            if not os.path.exists(dst_drawn_path):\n",
    "                shutil.copy(file_path, dst_drawn_path)\n",
    "                print(f\"âœ… Copied: {fname} to {drawn_dir}\")\n",
    "\n",
    "            # Copy corresponding original image if not already copied\n",
    "            src_original_path = os.path.join(base_dir, original_fname)\n",
    "            dst_original_path = os.path.join(original_dir, original_fname)\n",
    "            if os.path.exists(src_original_path) and not os.path.exists(dst_original_path):\n",
    "                shutil.copy(src_original_path, dst_original_path)\n",
    "                print(f\"âœ… Copied: {original_fname} to {original_dir}\")\n",
    "\n",
    "        # Process standalone original images\n",
    "        elif \"_original\" in fname:\n",
    "            dst_original_path = os.path.join(original_dir, fname)\n",
    "            if not os.path.exists(dst_original_path):\n",
    "                shutil.copy(file_path, dst_original_path)\n",
    "                print(f\"âœ… Copied: {fname} to {original_dir}\")"
   ],
   "metadata": {
    "id": "PIlPnNCAXn-c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "expected_train_original = 853\n",
    "expected_train_drawn = 853\n",
    "expected_val_original = 214\n",
    "expected_val_drawn = 214\n",
    "expected_test_original = 458\n",
    "expected_test_drawn = 458\n",
    "\n",
    "# Calling the function for each set\n",
    "create_subfolders_and_copy_files(normalized_train_dir, expected_train_original, expected_train_drawn)\n",
    "create_subfolders_and_copy_files(val_dir, expected_val_original, expected_val_drawn)\n",
    "create_subfolders_and_copy_files(normalized_test_dir, expected_test_original, expected_test_drawn)\n",
    "\n",
    "print(\"âœ… All files have been checked and copied if needed.\")"
   ],
   "metadata": {
    "id": "QJkKEBroXqHd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_image_paths(dir_path):\n",
    "    return [\n",
    "        os.path.join(dir_path, fname)\n",
    "        for fname in sorted(os.listdir(dir_path))\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))  # handles common formats\n",
    "    ]\n",
    "\n",
    "\n",
    "train_drawn_paths = get_image_paths(os.path.join(normalized_train_dir, \"drawn\"))\n",
    "train_original_paths = get_image_paths(os.path.join(normalized_train_dir, \"original\"))\n",
    "\n",
    "val_drawn_paths = get_image_paths(os.path.join(val_dir, \"drawn\"))\n",
    "val_original_paths = get_image_paths(os.path.join(val_dir, \"original\"))\n",
    "\n",
    "test_drawn_paths = get_image_paths(os.path.join(normalized_test_dir, \"drawn\"))\n",
    "test_original_paths = get_image_paths(os.path.join(normalized_test_dir, \"original\"))"
   ],
   "metadata": {
    "id": "5PwhTpbKXr9H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Train drawn: {len(train_drawn_paths)}\")\n",
    "print(f\"Train original: {len(train_original_paths)}\")\n",
    "\n",
    "print(f\"Val drawn: {len(val_drawn_paths)}\")\n",
    "print(f\"Val original: {len(val_original_paths)}\")\n",
    "\n",
    "print(f\"Test drawn: {len(test_drawn_paths)}\")\n",
    "print(f\"Test original: {len(test_original_paths)}\")"
   ],
   "metadata": {
    "id": "Wwg7bPdKXtpK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def load_image_pair(drawn_path, original_path):\n",
    "    def _decode_and_preprocess(path):\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1] instead of casting\n",
    "        image = tf.image.resize(image, [512,512])\n",
    "        return image\n",
    "\n",
    "    drawn_image = _decode_and_preprocess(drawn_path)\n",
    "    original_image = _decode_and_preprocess(original_path)\n",
    "\n",
    "    # Apply augmentation with probability\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        drawn_image = tf.image.flip_left_right(drawn_image)\n",
    "        original_image = tf.image.flip_left_right(original_image)\n",
    "\n",
    "    if tf.random.uniform([]) > 0.7:\n",
    "        drawn_image = tf.image.random_brightness(drawn_image, max_delta=0.1)\n",
    "        drawn_image = tf.image.random_contrast(drawn_image, lower=0.9, upper=1.1)\n",
    "\n",
    "    # Normalize to [-1, 1]\n",
    "    drawn_image = (drawn_image * 2.0) - 1.0\n",
    "    original_image = (original_image * 2.0) - 1.0\n",
    "\n",
    "    return drawn_image, original_image"
   ],
   "metadata": {
    "id": "l67Abx1cXwx6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def augment_data(input_image, target_image):\n",
    "    # Random jitter (resize and crop)\n",
    "    stacked = tf.concat([input_image, target_image], axis=2)\n",
    "\n",
    "    # Random flip\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        stacked = tf.image.flip_left_right(stacked)\n",
    "\n",
    "    # Split back\n",
    "    input_image = stacked[:, :, :3]\n",
    "    target_image = stacked[:, :, 3:]\n",
    "\n",
    "    # Adjust brightness/contrast for sketch only\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.random_brightness(input_image, 0.2)\n",
    "\n",
    "    # Optional: random cropping with resize\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Get dimensions\n",
    "        height = tf.shape(input_image)[0]\n",
    "        width = tf.shape(input_image)[1]\n",
    "\n",
    "        # Random crop size (80-100% of original size)\n",
    "        crop_size = tf.random.uniform(\n",
    "            [],\n",
    "            minval=tf.cast(0.8 * tf.cast(tf.minimum(height, width), tf.float32), tf.int32),\n",
    "            maxval=tf.cast(tf.minimum(height, width), tf.int32),\n",
    "            dtype=tf.int32\n",
    "        )\n",
    "\n",
    "        # Apply the same crop to both images\n",
    "        stacked = tf.concat([input_image, target_image], axis=2)\n",
    "        cropped = tf.image.random_crop(stacked, [crop_size, crop_size, 6])\n",
    "\n",
    "        # Split and resize back to original dimensions\n",
    "        input_image = tf.image.resize(cropped[:, :, :3], [256, 256])\n",
    "        target_image = tf.image.resize(cropped[:, :, 3:], [256, 256])\n",
    "\n",
    "    return input_image, target_image"
   ],
   "metadata": {
    "id": "1AGgmbcWXzBA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "# Function to create dataset from file paths\n",
    "def build_dataset(drawn_paths, original_paths, training=True):\n",
    "    # Ensure paths are in list of strings\n",
    "    drawn_paths = [str(p) for p in drawn_paths]\n",
    "    original_paths = [str(p) for p in original_paths]\n",
    "\n",
    "    # Ensure the paths are the same length\n",
    "    assert len(drawn_paths) == len(original_paths), \"Drawn and original paths must have the same length\"\n",
    "\n",
    "    # Create tf.data.Dataset from the file paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((drawn_paths, original_paths))\n",
    "\n",
    "    # Map the image loading function\n",
    "    dataset = dataset.map(load_image_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Shuffle the dataset only for training\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
    "\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    # Prefetch for performance\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ],
   "metadata": {
    "id": "PgIxXqYuXzx4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build datasets for train, validation, and test\n",
    "train_dataset = build_dataset(train_drawn_paths, train_original_paths, training=True)\n",
    "val_dataset = build_dataset(val_drawn_paths, val_original_paths, training=False)\n",
    "test_dataset = build_dataset(test_drawn_paths, test_original_paths, training=False)\n",
    "\n",
    "# Example usage: Iterate through the train dataset\n",
    "for images in train_dataset.take(1):\n",
    "    drawn_images, original_images = images\n",
    "    print(f\"Drawn image batch shape: {drawn_images.shape}\")\n",
    "    print(f\"Original image batch shape: {original_images.shape}\")\n",
    "\n",
    "print(\"âœ… Datasets created for train, validation, and test.\")"
   ],
   "metadata": {
    "id": "7D2PhN_fX1dt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Defined"
   ],
   "metadata": {
    "id": "yTJ7kvcyYtJ0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, DDPMScheduler, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings"
   ],
   "metadata": {
    "id": "1MwK9Kl_X2_j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Suppress some warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"  # Base Stable Diffusion model\n",
    "    output_dir = \"sketch_to_image_model\"\n",
    "    resolution = 512 \n",
    "    train_batch_size = 8\n",
    "    val_batch_size = 8\n",
    "    num_train_epochs = 18\n",
    "    gradient_accumulation_steps = 2\n",
    "    learning_rate = 1e-4\n",
    "    lr_scheduler = \"constant\"\n",
    "    lr_warmup_steps = 0\n",
    "    mixed_precision = \"no\"  # Using full precision to avoid dtype issues\n",
    "    save_images_epochs = 1\n",
    "    save_model_epochs = 1\n",
    "    validation_steps = 50\n",
    "\n",
    "config = Config()"
   ],
   "metadata": {
    "id": "vRUwF4wWX4il"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def add_error_handling():\n",
    "    def handle_tensor_error(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except RuntimeError as e:\n",
    "                if \"expected scalar type\" in str(e) or \"type\" in str(e):\n",
    "                    print(f\"Data type mismatch error detected: {e}\")\n",
    "                    print(\"Input tensor types:\")\n",
    "                    for i, arg in enumerate(args):\n",
    "                        if isinstance(arg, torch.Tensor):\n",
    "                            print(f\"Arg {i}: {arg.dtype}\")\n",
    "                    for k, v in kwargs.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            print(f\"Kwarg {k}: {v.dtype}\")\n",
    "                    raise\n",
    "                else:\n",
    "                    raise\n",
    "        return wrapper\n",
    "\n",
    "    # Patch key functions to add error handling\n",
    "    original_conv2d = torch.nn.functional.conv2d\n",
    "    torch.nn.functional.conv2d = handle_tensor_error(original_conv2d)"
   ],
   "metadata": {
    "id": "i_BNKnqqX6NH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create dataset class with improved prompts\n",
    "class SketchToImageDataset(Dataset):\n",
    "    def __init__(self, sketch_paths, image_paths, tokenizer, transform=None, prompt_engineering=True):\n",
    "        self.sketch_paths = sketch_paths\n",
    "        self.image_paths = image_paths\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_engineering = prompt_engineering\n",
    "\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((config.resolution, config.resolution)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "            ])\n",
    "\n",
    "        # Using a more descriptive prompt to guide the model better\n",
    "        self.default_prompt = \"a detailed, high-quality photograph of furniture generated from sketch\"\n",
    "\n",
    "        self.furniture_prompts = [\n",
    "            \"a detailed, high-quality photograph of furniture\",\n",
    "            \"a professional photograph of home furniture in natural lighting\",\n",
    "            \"a realistic photograph of furniture with clean details\",\n",
    "            \"a high-resolution image of furniture from sketch\",\n",
    "            \"a photorealistic rendering of furniture design\",\n",
    "            \"a detailed furniture photograph with accurate textures and materials\",\n",
    "            \"a professional product photograph of furniture piece\",\n",
    "            \"a clear, detailed image of furniture with realistic details\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sketch_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load sketch and target image\n",
    "        sketch = Image.open(self.sketch_paths[idx]).convert(\"RGB\")\n",
    "        target_image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "\n",
    "        # Apply transformations\n",
    "        sketch_tensor = self.transform(sketch)\n",
    "        target_tensor = self.transform(target_image)\n",
    "\n",
    "        # Select a prompt (either fixed or random from collection)\n",
    "        if self.prompt_engineering:\n",
    "            prompt = np.random.choice(self.furniture_prompts)\n",
    "        else:\n",
    "            prompt = self.default_prompt\n",
    "\n",
    "        # Encode the text prompt\n",
    "        text_inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids[0]\n",
    "\n",
    "        return {\n",
    "            \"sketch\": sketch_tensor,\n",
    "            \"target\": target_tensor,\n",
    "            \"input_ids\": text_input_ids,\n",
    "            \"file_path\": self.image_paths[idx]  \n",
    "        }"
   ],
   "metadata": {
    "id": "__3Qn6S3X7zq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Helper function to create directories if they don't exist\n",
    "def create_directories():\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(config.output_dir, \"samples\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(config.output_dir, \"checkpoints\"), exist_ok=True)\n",
    "\n",
    "# Function to save sample images during training\n",
    "def save_samples(controlnet, unet, vae, text_encoder, tokenizer, noise_scheduler, sketch_batch, epoch, device, show_images=True):\n",
    "    # Create pipeline for inference\n",
    "    pipeline = StableDiffusionControlNetPipeline(\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        controlnet=controlnet,\n",
    "        scheduler=noise_scheduler,\n",
    "        safety_checker=None,\n",
    "        feature_extractor=None,\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "\n",
    "    # Move to device\n",
    "    pipeline = pipeline.to(device)\n",
    "\n",
    "    # Get a few sketch samples\n",
    "    sketches = sketch_batch[\"sketch\"].to(device)\n",
    "    targets = sketch_batch[\"target\"].to(device)\n",
    "    num_samples = min(4, len(sketches))\n",
    "\n",
    "    sketches = sketches[:num_samples]\n",
    "    targets = targets[:num_samples]\n",
    "\n",
    "    # Prompt\n",
    "    prompt = [\"a detailed, high-quality photograph generated from a sketch\"] * num_samples\n",
    "\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        images = pipeline(\n",
    "            prompt=prompt,\n",
    "            image=sketches,\n",
    "            num_inference_steps=100,\n",
    "            guidance_scale=10.0\n",
    "        ).images\n",
    "\n",
    "    # Convert tensors and images\n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sketch_img = (sketches[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5) * 255\n",
    "        sketch_img = sketch_img.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        target_img = (targets[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5) * 255\n",
    "        target_img = target_img.clip(0, 255).astype(np.uint8)\n",
    "\n",
    "        if num_samples > 1:\n",
    "            axes[i, 0].imshow(sketch_img)\n",
    "            axes[i, 0].set_title(\"Sketch Input\")\n",
    "            axes[i, 0].axis(\"off\")\n",
    "\n",
    "            axes[i, 1].imshow(images[i])\n",
    "            axes[i, 1].set_title(\"Generated Image\")\n",
    "            axes[i, 1].axis(\"off\")\n",
    "\n",
    "            axes[i, 2].imshow(target_img)\n",
    "            axes[i, 2].set_title(\"Target Image\")\n",
    "            axes[i, 2].axis(\"off\")\n",
    "        else:\n",
    "            axes[0].imshow(sketch_img)\n",
    "            axes[0].set_title(\"Sketch Input\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].imshow(images[i])\n",
    "            axes[1].set_title(\"Generated Image\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            axes[2].imshow(target_img)\n",
    "            axes[2].set_title(\"Target Image\")\n",
    "            axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"samples\", f\"sample_epoch_{epoch}.png\"))\n",
    "\n",
    "    if show_images:\n",
    "        plt.show()  \n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return images"
   ],
   "metadata": {
    "id": "ciG972f-X9qv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Modified train_controlnet function with fixed model prediction handling\n",
    "def train_controlnet(train_sketch_paths, train_image_paths, val_sketch_paths, val_image_paths):\n",
    "    create_directories()\n",
    "\n",
    "    # Add custom error handling\n",
    "    add_error_handling()\n",
    "\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config.mixed_precision,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
    "    )\n",
    "\n",
    "    # Initialize logging\n",
    "    accelerator.init_trackers(\"controlnet_training\")\n",
    "\n",
    "    # Load the tokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(config.pretrained_model_name, subfolder=\"tokenizer\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SketchToImageDataset(train_sketch_paths, train_image_paths, tokenizer)\n",
    "    val_dataset = SketchToImageDataset(val_sketch_paths, val_image_paths, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.train_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.val_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    # Load models\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(config.pretrained_model_name, subfolder=\"scheduler\")\n",
    "\n",
    "    # Load the VAE component\n",
    "    from diffusers import AutoencoderKL\n",
    "    vae = AutoencoderKL.from_pretrained(config.pretrained_model_name, subfolder=\"vae\")\n",
    "\n",
    "    text_encoder = CLIPTextModel.from_pretrained(config.pretrained_model_name, subfolder=\"text_encoder\")\n",
    "\n",
    "    # Load the UNet\n",
    "    unet = UNet2DConditionModel.from_pretrained(config.pretrained_model_name, subfolder=\"unet\")\n",
    "\n",
    "    # Create a ControlNet model from the UNet\n",
    "    controlnet = ControlNetModel.from_unet(unet)\n",
    "\n",
    "    # Freeze vae and text_encoder\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)  # We only train ControlNet\n",
    "\n",
    "    # Optimizer for ControlNet with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        controlnet.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=1e-2  \n",
    "    )\n",
    "\n",
    "    # Calculate total number of training steps\n",
    "    total_train_batch_size = config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps\n",
    "    num_update_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps\n",
    "    max_train_steps = config.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=config.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=config.lr_warmup_steps * accelerator.num_processes,\n",
    "        num_training_steps=max_train_steps,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with accelerator\n",
    "    controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # For mixed precision training, we cast the models to the appropriate precision\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Cast all models to the correct dtype\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    controlnet.to(weight_dtype)\n",
    "\n",
    "    # We need to keep vae, unet and text_encoder in eval mode\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    unet.eval()\n",
    "\n",
    "    # Set controlnet to train mode\n",
    "    controlnet.train()\n",
    "\n",
    "    # Keep track of losses\n",
    "    global_step = 0\n",
    "    best_loss = float('inf')\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(config.num_train_epochs):\n",
    "        controlnet.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(controlnet):\n",
    "                sketch = batch[\"sketch\"].to(accelerator.device, dtype=weight_dtype)\n",
    "                target = batch[\"target\"].to(accelerator.device, dtype=weight_dtype)\n",
    "                input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
    "\n",
    "                # Encode target image to latent space\n",
    "                latents = vae.encode(target).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise and add to latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Encode text\n",
    "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "                controlnet_output = controlnet(noisy_latents, timesteps, encoder_hidden_states, sketch,return_dict=True)\n",
    "                unet_output = unet(\n",
    "                    sample=noisy_latents,\n",
    "                    timestep=timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    down_block_additional_residuals=controlnet_output.down_block_res_samples,\n",
    "                    mid_block_additional_residual=controlnet_output.mid_block_res_sample,\n",
    "                    return_dict=True)\n",
    "\n",
    "                # Extract the noise prediction from the model output\n",
    "                model_pred = unet_output.sample \n",
    "\n",
    "                # For debugging\n",
    "                if step == 0 and epoch == 0:\n",
    "                    print(f\"Model output type: {type(unet_output)}\")\n",
    "                    print(f\"Model prediction type: {type(model_pred)}\")\n",
    "                    print(f\"Model prediction shape: {model_pred.shape}\")\n",
    "                    print(f\"Noise shape: {noise.shape}\")\n",
    "\n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if global_step % config.validation_steps == 0:\n",
    "                accelerator.print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "            if global_step % 100 == 0:  # Every 100 steps, log the loss\n",
    "                print(f\"Epoch [{epoch + 1}/{config.num_train_epochs}], Step [{global_step}], Loss: {total_loss / (step + 1)}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        accelerator.print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save the best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_model_path = os.path.join(config.output_dir, \"checkpoints\", f\"best_model_epoch_{epoch}\")\n",
    "            accelerator.save_model(controlnet, best_model_path)\n",
    "            print(f\"New best model saved at epoch {epoch} with loss {best_loss:.4f}\")\n",
    "\n",
    "        # Save images and model\n",
    "        if epoch % config.save_images_epochs == 0:\n",
    "            val_batch = next(iter(val_dataloader))\n",
    "            save_samples(controlnet, unet, vae, text_encoder, tokenizer, noise_scheduler, val_batch, epoch, accelerator.device)\n",
    "\n",
    "            # Inline image display\n",
    "            try:\n",
    "                from IPython.display import Image as IPyImage, display\n",
    "                sample_path = os.path.join(config.output_dir, \"samples\", f\"sample_epoch_{epoch}.png\")\n",
    "                display(IPyImage(filename=sample_path))\n",
    "            except ImportError:\n",
    "                print(f\"Sample image saved at {sample_path}\")\n",
    "\n",
    "        if epoch % config.save_model_epochs == 0:\n",
    "            model_path = os.path.join(config.output_dir, \"checkpoints\", f\"controlnet_epoch_{epoch}\")\n",
    "            accelerator.save_model(controlnet, model_path)\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "    # Return the path to the best model\n",
    "    if best_model_path is None:\n",
    "        best_model_path = os.path.join(config.output_dir, \"checkpoints\", f\"controlnet_epoch_{config.num_train_epochs-1}\")\n",
    "\n",
    "    return best_model_path"
   ],
   "metadata": {
    "id": "sVert7yoX_vo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "\n",
    "# Function to run inference with trained model\n",
    "def run_inference(model_path, test_sketch_paths, test_image_paths, num_samples=5):\n",
    "   \n",
    "    print(f\"Running inference with model from: {model_path}\")\n",
    "    \n",
    "    # Set the torch dtype consistently\n",
    "    torch_dtype = torch.float16  \n",
    "    \n",
    "    # Initialize the base models from pretrained\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        Config.pretrained_model_name,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False,\n",
    "        controlnet=None,\n",
    "        torch_dtype=torch_dtype  \n",
    "    )\n",
    "    \n",
    "    # Load the saved controlnet weights manually\n",
    "    try:\n",
    "        controlnet = ControlNetModel.from_pretrained(model_path, torch_dtype=torch_dtype)\n",
    "        pipe.controlnet = controlnet\n",
    "    except (OSError, ValueError) as e:\n",
    "        print(f\"Could not load model directly: {e}\")\n",
    "        print(\"Attempting to load model weights manually...\")\n",
    "        \n",
    "        # Create a new controlnet from the base model\n",
    "        controlnet = ControlNetModel.from_unet(pipe.unet)\n",
    "        \n",
    "        try:\n",
    "            model_files = [f for f in os.listdir(model_path) if f.endswith('.bin') or f.endswith('.pt')]\n",
    "            \n",
    "            if model_files:\n",
    "                print(f\"Found model files: {model_files}\")\n",
    "                state_dict = torch.load(os.path.join(model_path, model_files[0]), map_location=\"cpu\")\n",
    "                \n",
    "                # Check if we got a state_dict directly or need to extract it\n",
    "                if not isinstance(state_dict, dict) or \"state_dict\" in state_dict:\n",
    "                    if \"state_dict\" in state_dict:\n",
    "                        state_dict = state_dict[\"state_dict\"]\n",
    "                    elif hasattr(state_dict, \"state_dict\"):\n",
    "                        state_dict = state_dict.state_dict()\n",
    "                \n",
    "                # Filter out unwanted keys if necessary\n",
    "                filtered_state_dict = {k: v for k, v in state_dict.items() \n",
    "                                      if k.startswith('controlnet.') or not k.startswith('model.')}\n",
    "                \n",
    "                # Load the state dict\n",
    "                missing, unexpected = controlnet.load_state_dict(filtered_state_dict, strict=False)\n",
    "                \n",
    "                if missing:\n",
    "                    print(f\"Missing keys: {missing[:5]}{'...' if len(missing) > 5 else ''}\")\n",
    "                if unexpected:\n",
    "                    print(f\"Unexpected keys: {unexpected[:5]}{'...' if len(unexpected) > 5 else ''}\")\n",
    "                    \n",
    "                # Explicitly convert to the correct dtype\n",
    "                controlnet = controlnet.to(torch_dtype)\n",
    "                pipe.controlnet = controlnet\n",
    "                print(\"Successfully loaded model weights manually.\")\n",
    "            else:\n",
    "                print(f\"No model files found in {model_path}. Looking for safetensors...\")\n",
    "                \n",
    "                # Look for safetensors files\n",
    "                safetensors_files = [f for f in os.listdir(model_path) if f.endswith('.safetensors')]\n",
    "                \n",
    "                if safetensors_files:\n",
    "                    print(f\"Found safetensors files: {safetensors_files}\")\n",
    "                    # Use from_pretrained with the safetensors file\n",
    "                    controlnet = ControlNetModel.from_pretrained(\n",
    "                        Config.pretrained_model_name, \n",
    "                        subfolder=\"controlnet\", \n",
    "                        resume_download=True,\n",
    "                        local_files_only=False,\n",
    "                        torch_dtype=torch_dtype\n",
    "                    )\n",
    "                    \n",
    "                    # Try to load the state dict\n",
    "                    from safetensors import safe_open\n",
    "                    safetensors_path = os.path.join(model_path, safetensors_files[0])\n",
    "                    with safe_open(safetensors_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "                        state_dict = {k: f.get_tensor(k) for k in f.keys()}\n",
    "                    \n",
    "                    missing, unexpected = controlnet.load_state_dict(state_dict, strict=False)\n",
    "                    pipe.controlnet = controlnet.to(torch_dtype)\n",
    "                    print(\"Successfully loaded model from safetensors.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"No model files found in {model_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load model weights manually: {e}\")\n",
    "            print(\"Falling back to using base ControlNet model...\")\n",
    "            controlnet = ControlNetModel.from_pretrained(\n",
    "                \"lllyasviel/sd-controlnet-canny\",  # Use a pretrained ControlNet as fallback\n",
    "                torch_dtype=torch_dtype\n",
    "            )\n",
    "            pipe.controlnet = controlnet\n",
    "    \n",
    "    # Move model to GPU\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    \n",
    "    # Use DDPM scheduler for better quality\n",
    "    pipe.scheduler = DDPMScheduler.from_pretrained(Config.pretrained_model_name, subfolder=\"scheduler\")\n",
    "    \n",
    "    # Create a directory for the results\n",
    "    results_dir = \"inference_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Randomly sample test sketches\n",
    "    indices = np.random.choice(len(test_sketch_paths), min(num_samples, len(test_sketch_paths)), replace=False)\n",
    "    \n",
    "    # Create a transform for preprocessing sketches that matches the model's expected dtype\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config.resolution, config.resolution)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Create a counter for successful generations\n",
    "    successful_samples = 0\n",
    "    all_comparison_figs = []\n",
    "    \n",
    "    # Generate images for each sample\n",
    "    for i, idx in enumerate(indices):\n",
    "        # Load sketch and target image\n",
    "        sketch_path = test_sketch_paths[idx]\n",
    "        target_path = test_image_paths[idx]\n",
    "        \n",
    "        print(f\"Processing sketch: {sketch_path}\")\n",
    "        print(f\"Target image: {target_path}\")\n",
    "        \n",
    "        # Load sketch and target image\n",
    "        sketch = Image.open(sketch_path).convert(\"RGB\")\n",
    "        target = Image.open(target_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize both for visualization\n",
    "        sketch = sketch.resize((config.resolution, config.resolution))\n",
    "        target = target.resize((config.resolution, config.resolution))\n",
    "        \n",
    "        # Generate image\n",
    "        prompt = \"a detailed, high-quality photograph generated from a sketch\"\n",
    "        try:\n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                image=sketch,\n",
    "                num_inference_steps=50,\n",
    "                guidance_scale=7.5\n",
    "            ).images[0]\n",
    "            \n",
    "            # Save individual images for comparison\n",
    "            sketch.save(os.path.join(results_dir, f\"sample_{successful_samples}_sketch.png\"))\n",
    "            image.save(os.path.join(results_dir, f\"sample_{successful_samples}_generated.png\"))\n",
    "            target.save(os.path.join(results_dir, f\"sample_{successful_samples}_target.png\"))\n",
    "            \n",
    "            # Save the results\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            axes[0].imshow(sketch)\n",
    "            axes[0].set_title(\"Input Sketch\")\n",
    "            axes[0].axis(\"off\")\n",
    "            \n",
    "            axes[1].imshow(image)\n",
    "            axes[1].set_title(\"Generated Image\")\n",
    "            axes[1].axis(\"off\")\n",
    "            \n",
    "            axes[2].imshow(target)\n",
    "            axes[2].set_title(\"Ground Truth\")\n",
    "            axes[2].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, f\"comparison_{successful_samples}.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            all_comparison_figs.append(fig)\n",
    "            successful_samples += 1\n",
    "            \n",
    "            print(f\"Saved comparison image: comparison_{successful_samples-1}.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image {i}: {e}\")\n",
    "            print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "    \n",
    "    # Create a summary image with all comparisons\n",
    "    if successful_samples > 0:\n",
    "        try:\n",
    "            fig, axes = plt.subplots(successful_samples, 3, figsize=(15, 5 * successful_samples))\n",
    "            \n",
    "            for i in range(successful_samples):\n",
    "                try:\n",
    "                    sketch = Image.open(os.path.join(results_dir, f\"sample_{i}_sketch.png\"))\n",
    "                    generated = Image.open(os.path.join(results_dir, f\"sample_{i}_generated.png\"))\n",
    "                    target = Image.open(os.path.join(results_dir, f\"sample_{i}_target.png\"))\n",
    "                    \n",
    "                    if successful_samples > 1:\n",
    "                        axes[i, 0].imshow(sketch)\n",
    "                        axes[i, 0].set_title(f\"Sketch {i+1}\")\n",
    "                        axes[i, 0].axis(\"off\")\n",
    "                        \n",
    "                        axes[i, 1].imshow(generated)\n",
    "                        axes[i, 1].set_title(f\"Generated {i+1}\")\n",
    "                        axes[i, 1].axis(\"off\")\n",
    "                        \n",
    "                        axes[i, 2].imshow(target)\n",
    "                        axes[i, 2].set_title(f\"Target {i+1}\")\n",
    "                        axes[i, 2].axis(\"off\")\n",
    "                    else:\n",
    "                        axes[0].imshow(sketch)\n",
    "                        axes[0].set_title(\"Sketch\")\n",
    "                        axes[0].axis(\"off\")\n",
    "                        \n",
    "                        axes[1].imshow(generated)\n",
    "                        axes[1].set_title(\"Generated\")\n",
    "                        axes[1].axis(\"off\")\n",
    "                        \n",
    "                        axes[2].imshow(target)\n",
    "                        axes[2].set_title(\"Target\")\n",
    "                        axes[2].axis(\"off\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not include image {i} in summary: {e}\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(results_dir, \"all_comparisons.png\"))\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"All inference results saved to: {results_dir}\")\n",
    "            print(f\"Summary image saved as: {os.path.join(results_dir, 'all_comparisons.png')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating summary image: {e}\")\n",
    "            print(f\"Detailed error: {traceback.format_exc()}\")\n",
    "    else:\n",
    "        print(\"No successful image generations to create summary.\")\n",
    "\n",
    "    return results_dir"
   ],
   "metadata": {
    "id": "alc7xHKQYCLJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_sketch_paths = train_drawn_paths\n",
    "    train_image_paths = train_original_paths\n",
    "    val_sketch_paths = val_drawn_paths\n",
    "    val_image_paths = val_original_paths\n",
    "    test_sketch_paths = test_drawn_paths\n",
    "    test_image_paths = test_original_paths\n",
    "\n",
    "    # Train the model\n",
    "    best_model_path = train_controlnet(train_sketch_paths, train_image_paths, val_sketch_paths, val_image_paths)\n",
    "\n",
    "    # Run inference with the best model (not just the final model)\n",
    "    run_inference(\n",
    "        best_model_path,  \n",
    "        test_sketch_paths,\n",
    "        test_image_paths,\n",
    "        num_samples=5,\n",
    "    )"
   ],
   "metadata": {
    "id": "qjNt99GIYEju"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
