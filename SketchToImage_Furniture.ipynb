{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSR6_2eYRNGZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "dataset_dir = os.path.join(base_path, \"dataset furniture\")\n",
        "\n",
        "filenames = os.listdir(dataset_dir)\n",
        "print(f\"ðŸ“ Total files in folder: {len(filenames)}\")"
      ],
      "metadata": {
        "id": "i1YLX-60RPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drawn_ids = set()\n",
        "original_ids = set()\n",
        "\n",
        "for fname in filenames:\n",
        "    if \"_drawn\" in fname:\n",
        "        image_id = fname.replace(\"fimage_\", \"\").split(\"_drawn\")[0]\n",
        "        drawn_ids.add(image_id)\n",
        "\n",
        "    elif \"_original\" in fname:\n",
        "        image_id = fname.replace(\"fimage_\", \"\").split(\"_original\")[0]\n",
        "        original_ids.add(image_id)\n",
        "\n",
        "paired_ids = drawn_ids & original_ids"
      ],
      "metadata": {
        "id": "m1W9yH4nRQis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ðŸ–Œï¸ Total sketch images: {len(drawn_ids)}\")\n",
        "print(f\"ðŸ–¼ï¸ Total original images: {len(original_ids)}\")\n",
        "print(f\"âœ… Found {len(paired_ids)} sketch-original image pairs.\")"
      ],
      "metadata": {
        "id": "N9KaQaydRRzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train Test split"
      ],
      "metadata": {
        "id": "3RhRzCPtRT-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Index all files by ID and type (drawn / original)\n",
        "file_mapping = {}\n",
        "\n",
        "for fname in filenames:\n",
        "    if not fname.startswith(\"fimage\"):\n",
        "        continue\n",
        "\n",
        "    if \"_drawn\" in fname:\n",
        "        image_id = fname.replace(\"fimage\", \"\").split(\"_drawn\")[0]\n",
        "        file_mapping.setdefault(image_id, {})['drawn'] = fname\n",
        "    elif \"_original\" in fname:\n",
        "        image_id = fname.replace(\"fimage\", \"\").split(\"_original\")[0]\n",
        "        file_mapping.setdefault(image_id, {})['original'] = fname\n",
        "\n",
        "# Step 2: Filter to only include complete pairs\n",
        "paired_ids = sorted([img_id for img_id, pair in file_mapping.items() if 'drawn' in pair and 'original' in pair])\n",
        "\n",
        "# Step 3: Train/test split\n",
        "split_idx = int(0.7 * len(paired_ids))\n",
        "train_ids = paired_ids[:split_idx]\n",
        "test_ids = paired_ids[split_idx:]\n",
        "\n",
        "# Step 4: Build train and test filenames\n",
        "train_filenames = []\n",
        "test_filenames = []\n",
        "\n",
        "for image_id in train_ids:\n",
        "    pair = file_mapping[image_id]\n",
        "    train_filenames.append(pair['drawn'])\n",
        "    train_filenames.append(pair['original'])\n",
        "\n",
        "for image_id in test_ids:\n",
        "    pair = file_mapping[image_id]\n",
        "    test_filenames.append(pair['drawn'])\n",
        "    test_filenames.append(pair['original'])\n",
        "\n",
        "print(f\"ðŸ›¤ï¸ Train set size: {len(train_filenames)} images\")\n",
        "print(f\"ðŸ§ª Test set size: {len(test_filenames)} images\")"
      ],
      "metadata": {
        "id": "Sus5NtKyRWkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source and destination directories\n",
        "train_dir = os.path.join(base_path, \"dataset furniture\", \"train\")\n",
        "test_dir = os.path.join(base_path, \"dataset furniture\", \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy files if not already present\n",
        "def copy_files(file_list, src_dir, dst_dir):\n",
        "    for fname in file_list:\n",
        "        src_path = os.path.join(src_dir, fname)\n",
        "        dst_path = os.path.join(dst_dir, fname)\n",
        "\n",
        "        # Only copy if the file doesn't exist in the destination\n",
        "        if not os.path.exists(dst_path):\n",
        "            shutil.copyfile(src_path, dst_path)\n",
        "            print(f\"Copied: {fname}\")  # You can remove this line if not needed for debugging\n",
        "        else:\n",
        "            print(f\"Skipped (already exists): {fname}\")  # Optional, for feedback\n",
        "\n",
        "# Copy train files only if they don't exist\n",
        "copy_files(train_filenames, dataset_dir, train_dir)\n",
        "\n",
        "# Copy test files only if they don't exist\n",
        "copy_files(test_filenames, dataset_dir, test_dir)\n",
        "\n",
        "print(\"âœ… Files successfully copied to train/ and test/ folders.\")"
      ],
      "metadata": {
        "id": "gANAeoIFRYFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre Processing"
      ],
      "metadata": {
        "id": "OSd_3CsEYQMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Resizing"
      ],
      "metadata": {
        "id": "pJWkssshYUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Function to get the size of an image\n",
        "def get_image_size(image_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        return img.size  # Returns (width, height)\n",
        "\n",
        "# Checking the size of a sample image in the dataset\n",
        "sample_image_path = os.path.join(dataset_dir, train_filenames[0])  # Use any file in the dataset\n",
        "image_size = get_image_size(sample_image_path)\n",
        "print(f\"Sample image size: {image_size}\")"
      ],
      "metadata": {
        "id": "hZemWQHjRZxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Normalization"
      ],
      "metadata": {
        "id": "KSIgEHw5YXbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Create a folder to save normalized images\n",
        "normalized_train_dir = os.path.join(dataset_dir, \"normalized_train\")\n",
        "normalized_test_dir = os.path.join(dataset_dir, \"normalized_test\")\n",
        "\n",
        "os.makedirs(normalized_train_dir, exist_ok=True)\n",
        "os.makedirs(normalized_test_dir, exist_ok=True)\n",
        "\n",
        "# Function to normalize image and save it\n",
        "def normalize_image(image_path, save_path):\n",
        "    with Image.open(image_path) as img:\n",
        "        img_array = np.array(img).astype(np.float32)\n",
        "        img_array = (img_array / 127.5) - 1.0\n",
        "        normalized_img = Image.fromarray(((img_array + 1.0) * 127.5).astype(np.uint8))\n",
        "        normalized_img.save(save_path)\n",
        "        print(f\"Saved normalized image to {save_path}\")\n",
        "\n",
        "# Normalize and save all images in the training and test sets\n",
        "# for fname in train_filenames:\n",
        "#     image_path = os.path.join(train_dir, fname)\n",
        "#     save_path = os.path.join(normalized_train_dir, fname)\n",
        "#     normalize_image(image_path, save_path)\n",
        "\n",
        "# for fname in test_filenames:\n",
        "#     image_path = os.path.join(test_dir, fname)\n",
        "#     save_path = os.path.join(normalized_test_dir, fname)\n",
        "#     normalize_image(image_path, save_path)\n",
        "\n",
        "def check_random_normalized_images(file_list, normalized_dir, num_samples=2):\n",
        "    samples = random.sample(file_list, min(num_samples, len(file_list)))\n",
        "    all_correct = True\n",
        "\n",
        "    for fname in samples:\n",
        "        normalized_path = os.path.join(normalized_dir, fname)\n",
        "        with Image.open(normalized_path) as img:\n",
        "            img_array = np.array(img)\n",
        "            if img_array.max() > 255 or img_array.min() < 0:\n",
        "                print(f\"âŒ Image {fname} has incorrect normalization values!\")\n",
        "                all_correct = False\n",
        "            else:\n",
        "                print(f\"âœ… Image {fname} appears correctly normalized.\")\n",
        "\n",
        "    if all_correct:\n",
        "        print(\"âœ… All images normalized and saved successfully!\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ Some images in {normalized_dir} may not be normalized correctly.\")\n",
        "\n",
        "check_random_normalized_images(train_filenames, normalized_train_dir)\n",
        "check_random_normalized_images(test_filenames, normalized_test_dir)"
      ],
      "metadata": {
        "id": "btLlDGCiXhPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Extract training image IDs (correctly)\n",
        "train_ids = [fname.replace(\"fimage\", \"\").split(\"_drawn\")[0] for fname in train_filenames if \"_drawn\" in fname]\n",
        "\n",
        "# Step 2: Split image IDs into train/val\n",
        "train_ids_split, val_ids_split = train_test_split(train_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Group filenames again by ID\n",
        "val_filenames = []\n",
        "new_train_filenames = []\n",
        "\n",
        "for image_id in train_ids_split:\n",
        "    if image_id in file_mapping:\n",
        "        pair = file_mapping[image_id]\n",
        "        new_train_filenames.append(pair['drawn'])\n",
        "        new_train_filenames.append(pair['original'])\n",
        "\n",
        "for image_id in val_ids_split:\n",
        "    if image_id in file_mapping:\n",
        "        pair = file_mapping[image_id]\n",
        "        val_filenames.append(pair['drawn'])\n",
        "        val_filenames.append(pair['original'])\n",
        "\n",
        "# âœ… Summary\n",
        "print(f\"ðŸ›¤ï¸ Train set: {len(new_train_filenames)} images\")\n",
        "print(f\"ðŸ§ª Validation set: {len(val_filenames)} images\")\n",
        "print(f\"ðŸ§ª Test set (unchanged): {len(test_filenames)} images\")"
      ],
      "metadata": {
        "id": "8M0KkreGXj_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define path for validation directory\n",
        "val_dir = os.path.join(dataset_dir, \"normalized_val\")\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Function to copy validation images\n",
        "def copy_files_if_needed(file_list, source_dir, destination_dir, expected_file_count):\n",
        "    # Check if the validation directory already has the expected number of files\n",
        "    existing_files = len([f for f in os.listdir(destination_dir) if os.path.isfile(os.path.join(destination_dir, f))])\n",
        "\n",
        "    if existing_files >= expected_file_count:\n",
        "        print(f\"â© Skipped copying: {existing_files} files already exist in {destination_dir}.\")\n",
        "        return  # Exit if the expected number of files are already present\n",
        "\n",
        "    for fname in file_list:\n",
        "        src_path = os.path.join(source_dir, fname)\n",
        "        dst_path = os.path.join(destination_dir, fname)\n",
        "\n",
        "        if not os.path.exists(src_path):\n",
        "            print(f\"âš ï¸ Warning: Source file not found: {src_path}\")\n",
        "        elif os.path.exists(dst_path):\n",
        "            print(f\"â© Skipped (already exists): {fname}\")\n",
        "        else:\n",
        "            shutil.copy(src_path, dst_path)\n",
        "            print(f\"âœ… Copied: {fname}\")\n",
        "\n",
        "# Expected number of files in validation set\n",
        "expected_val_file_count = 428\n",
        "\n",
        "# Copy validation files\n",
        "copy_files_if_needed(val_filenames, train_dir, val_dir, expected_val_file_count)\n",
        "\n",
        "print(\"âœ… Validation files checked and copied if needed.\")"
      ],
      "metadata": {
        "id": "pnXSuVxYXmHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preparation for training"
      ],
      "metadata": {
        "id": "1PNj7vJYYiOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to count the files in the subfolders\n",
        "def count_files_in_subfolders(base_dir):\n",
        "    original_dir = os.path.join(dataset_dir, \"original\")\n",
        "    drawn_dir = os.path.join(dataset_dir, \"drawn\")\n",
        "\n",
        "    num_original_files = len(os.listdir(original_dir)) if os.path.exists(original_dir) else 0\n",
        "    num_drawn_files = len(os.listdir(drawn_dir)) if os.path.exists(drawn_dir) else 0\n",
        "\n",
        "    return num_original_files, num_drawn_files\n",
        "\n",
        "# Function to create subfolders and copy files (if not already copied)\n",
        "def create_subfolders_and_copy_files(base_dir, expected_original_files, expected_drawn_files):\n",
        "    original_dir = os.path.join(base_dir, \"original\")\n",
        "    drawn_dir = os.path.join(base_dir, \"drawn\")\n",
        "\n",
        "    os.makedirs(original_dir, exist_ok=True)\n",
        "    os.makedirs(drawn_dir, exist_ok=True)\n",
        "\n",
        "    num_original_files, num_drawn_files = count_files_in_subfolders(base_dir)\n",
        "\n",
        "    if num_original_files >= expected_original_files and num_drawn_files >= expected_drawn_files:\n",
        "        print(f\"â© Skipping folder: {base_dir}, already contains enough original and drawn images.\")\n",
        "        return\n",
        "\n",
        "    # Loop through all files in the base directory (not subfolders)\n",
        "    for fname in os.listdir(base_dir):\n",
        "        file_path = os.path.join(base_dir, fname)\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            continue  # Skip if not a file\n",
        "\n",
        "        # Process drawn images\n",
        "        if \"_drawn\" in fname:\n",
        "            image_id = fname.replace(\"image_\", \"\").split(\"_drawn\")[0]\n",
        "            original_fname = f\"image_{image_id}_original.png\"\n",
        "\n",
        "            # Copy drawn image if not already copied\n",
        "            dst_drawn_path = os.path.join(drawn_dir, fname)\n",
        "            if not os.path.exists(dst_drawn_path):\n",
        "                shutil.copy(file_path, dst_drawn_path)\n",
        "                print(f\"âœ… Copied: {fname} to {drawn_dir}\")\n",
        "\n",
        "            # Copy corresponding original image if not already copied\n",
        "            src_original_path = os.path.join(base_dir, original_fname)\n",
        "            dst_original_path = os.path.join(original_dir, original_fname)\n",
        "            if os.path.exists(src_original_path) and not os.path.exists(dst_original_path):\n",
        "                shutil.copy(src_original_path, dst_original_path)\n",
        "                print(f\"âœ… Copied: {original_fname} to {original_dir}\")\n",
        "\n",
        "        # Process standalone original images\n",
        "        elif \"_original\" in fname:\n",
        "            dst_original_path = os.path.join(original_dir, fname)\n",
        "            if not os.path.exists(dst_original_path):\n",
        "                shutil.copy(file_path, dst_original_path)\n",
        "                print(f\"âœ… Copied: {fname} to {original_dir}\")"
      ],
      "metadata": {
        "id": "PIlPnNCAXn-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_train_original = 853\n",
        "expected_train_drawn = 853\n",
        "expected_val_original = 214\n",
        "expected_val_drawn = 214\n",
        "expected_test_original = 458\n",
        "expected_test_drawn = 458\n",
        "\n",
        "# Calling the function for each set\n",
        "create_subfolders_and_copy_files(normalized_train_dir, expected_train_original, expected_train_drawn)\n",
        "create_subfolders_and_copy_files(val_dir, expected_val_original, expected_val_drawn)\n",
        "create_subfolders_and_copy_files(normalized_test_dir, expected_test_original, expected_test_drawn)\n",
        "\n",
        "print(\"âœ… All files have been checked and copied if needed.\")"
      ],
      "metadata": {
        "id": "QJkKEBroXqHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths(dir_path):\n",
        "    return [\n",
        "        os.path.join(dir_path, fname)\n",
        "        for fname in sorted(os.listdir(dir_path))\n",
        "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\"))  # handles common formats\n",
        "    ]\n",
        "\n",
        "\n",
        "train_drawn_paths = get_image_paths(os.path.join(normalized_train_dir, \"drawn\"))\n",
        "train_original_paths = get_image_paths(os.path.join(normalized_train_dir, \"original\"))\n",
        "\n",
        "val_drawn_paths = get_image_paths(os.path.join(val_dir, \"drawn\"))\n",
        "val_original_paths = get_image_paths(os.path.join(val_dir, \"original\"))\n",
        "\n",
        "test_drawn_paths = get_image_paths(os.path.join(normalized_test_dir, \"drawn\"))\n",
        "test_original_paths = get_image_paths(os.path.join(normalized_test_dir, \"original\"))"
      ],
      "metadata": {
        "id": "5PwhTpbKXr9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train drawn: {len(train_drawn_paths)}\")\n",
        "print(f\"Train original: {len(train_original_paths)}\")\n",
        "\n",
        "print(f\"Val drawn: {len(val_drawn_paths)}\")\n",
        "print(f\"Val original: {len(val_original_paths)}\")\n",
        "\n",
        "print(f\"Test drawn: {len(test_drawn_paths)}\")\n",
        "print(f\"Test original: {len(test_original_paths)}\")"
      ],
      "metadata": {
        "id": "Wwg7bPdKXtpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def load_image_pair(drawn_path, original_path):\n",
        "    def _decode_and_preprocess(path):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)  # [0,1] instead of casting\n",
        "        image = tf.image.resize(image, [512,512])\n",
        "        return image\n",
        "\n",
        "    drawn_image = _decode_and_preprocess(drawn_path)\n",
        "    original_image = _decode_and_preprocess(original_path)\n",
        "\n",
        "    # Apply augmentation with probability\n",
        "    if tf.random.uniform([]) > 0.5:\n",
        "        drawn_image = tf.image.flip_left_right(drawn_image)\n",
        "        original_image = tf.image.flip_left_right(original_image)\n",
        "\n",
        "    if tf.random.uniform([]) > 0.7:\n",
        "        drawn_image = tf.image.random_brightness(drawn_image, max_delta=0.1)\n",
        "        drawn_image = tf.image.random_contrast(drawn_image, lower=0.9, upper=1.1)\n",
        "\n",
        "    # Normalize to [-1, 1]\n",
        "    drawn_image = (drawn_image * 2.0) - 1.0\n",
        "    original_image = (original_image * 2.0) - 1.0\n",
        "\n",
        "    return drawn_image, original_image"
      ],
      "metadata": {
        "id": "l67Abx1cXwx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(input_image, target_image):\n",
        "    # Random jitter (resize and crop)\n",
        "    stacked = tf.concat([input_image, target_image], axis=2)\n",
        "\n",
        "    # Random flip\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        stacked = tf.image.flip_left_right(stacked)\n",
        "\n",
        "    # Split back\n",
        "    input_image = stacked[:, :, :3]\n",
        "    target_image = stacked[:, :, 3:]\n",
        "\n",
        "    # Adjust brightness/contrast for sketch only\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.random_brightness(input_image, 0.2)\n",
        "\n",
        "    # Optional: random cropping with resize\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        # Get dimensions\n",
        "        height = tf.shape(input_image)[0]\n",
        "        width = tf.shape(input_image)[1]\n",
        "\n",
        "        # Random crop size (80-100% of original size)\n",
        "        crop_size = tf.random.uniform(\n",
        "            [],\n",
        "            minval=tf.cast(0.8 * tf.cast(tf.minimum(height, width), tf.float32), tf.int32),\n",
        "            maxval=tf.cast(tf.minimum(height, width), tf.int32),\n",
        "            dtype=tf.int32\n",
        "        )\n",
        "\n",
        "        # Apply the same crop to both images\n",
        "        stacked = tf.concat([input_image, target_image], axis=2)\n",
        "        cropped = tf.image.random_crop(stacked, [crop_size, crop_size, 6])\n",
        "\n",
        "        # Split and resize back to original dimensions\n",
        "        input_image = tf.image.resize(cropped[:, :, :3], [256, 256])\n",
        "        target_image = tf.image.resize(cropped[:, :, 3:], [256, 256])\n",
        "\n",
        "    return input_image, target_image"
      ],
      "metadata": {
        "id": "1AGgmbcWXzBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "BATCH_SIZE = 8\n",
        "SHUFFLE_BUFFER = 1000\n",
        "\n",
        "# Function to create dataset from file paths\n",
        "def build_dataset(drawn_paths, original_paths, training=True):\n",
        "    # Ensure paths are in list of strings\n",
        "    drawn_paths = [str(p) for p in drawn_paths]\n",
        "    original_paths = [str(p) for p in original_paths]\n",
        "\n",
        "    # Ensure the paths are the same length\n",
        "    assert len(drawn_paths) == len(original_paths), \"Drawn and original paths must have the same length\"\n",
        "\n",
        "    # Create tf.data.Dataset from the file paths\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((drawn_paths, original_paths))\n",
        "\n",
        "    # Map the image loading function\n",
        "    dataset = dataset.map(load_image_pair, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # Shuffle the dataset only for training\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(SHUFFLE_BUFFER)\n",
        "\n",
        "    # Batch the dataset\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "PgIxXqYuXzx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build datasets for train, validation, and test\n",
        "train_dataset = build_dataset(train_drawn_paths, train_original_paths, training=True)\n",
        "val_dataset = build_dataset(val_drawn_paths, val_original_paths, training=False)\n",
        "test_dataset = build_dataset(test_drawn_paths, test_original_paths, training=False)\n",
        "\n",
        "# Example usage: Iterate through the train dataset\n",
        "for images in train_dataset.take(1):\n",
        "    drawn_images, original_images = images\n",
        "    print(f\"Drawn image batch shape: {drawn_images.shape}\")\n",
        "    print(f\"Original image batch shape: {original_images.shape}\")\n",
        "\n",
        "print(\"âœ… Datasets created for train, validation, and test.\")"
      ],
      "metadata": {
        "id": "7D2PhN_fX1dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Defined"
      ],
      "metadata": {
        "id": "yTJ7kvcyYtJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import (\n",
        "    ControlNetModel,\n",
        "    UNet2DConditionModel,\n",
        "    DDPMScheduler,\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    get_scheduler\n",
        ")\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "from accelerate import Accelerator"
      ],
      "metadata": {
        "id": "1MwK9Kl_X2_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configuration with improved settings\n",
        "class Config:\n",
        "    pretrained_model_name = \"runwayml/stable-diffusion-v1-5\"  # Base Stable Diffusion model\n",
        "    output_dir = \"sketch_to_image_model\"\n",
        "    resolution = 512\n",
        "    train_batch_size = 8\n",
        "    val_batch_size = 8\n",
        "    num_train_epochs = 20\n",
        "    gradient_accumulation_steps = 2\n",
        "    learning_rate = 1e-4\n",
        "    lr_scheduler = \"constant\"\n",
        "    lr_warmup_steps = 0\n",
        "    mixed_precision = \"no\"\n",
        "    save_images_epochs = 1\n",
        "    save_model_epochs = 1\n",
        "    validation_steps = 50\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "vRUwF4wWX4il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_error_handling():\n",
        "    \"\"\"\n",
        "    Add custom error handling function to diagnose common issues\n",
        "    \"\"\"\n",
        "    def handle_tensor_error(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            except RuntimeError as e:\n",
        "                if \"expected scalar type\" in str(e) or \"type\" in str(e):\n",
        "                    print(f\"Data type mismatch error detected: {e}\")\n",
        "                    print(\"Input tensor types:\")\n",
        "                    for i, arg in enumerate(args):\n",
        "                        if isinstance(arg, torch.Tensor):\n",
        "                            print(f\"Arg {i}: {arg.dtype}\")\n",
        "                    for k, v in kwargs.items():\n",
        "                        if isinstance(v, torch.Tensor):\n",
        "                            print(f\"Kwarg {k}: {v.dtype}\")\n",
        "                    raise\n",
        "                else:\n",
        "                    raise\n",
        "        return wrapper\n",
        "\n",
        "    # Patch key functions to add error handling\n",
        "    original_conv2d = torch.nn.functional.conv2d\n",
        "    torch.nn.functional.conv2d = handle_tensor_error(original_conv2d)"
      ],
      "metadata": {
        "id": "i_BNKnqqX6NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset with improved prompts and consistent preprocessing\n",
        "class SketchToImageDataset(Dataset):\n",
        "    def __init__(self, sketch_paths, image_paths, tokenizer, prompt_engineering=True):\n",
        "        self.sketch_paths = sketch_paths\n",
        "        self.image_paths = image_paths\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prompt_engineering = prompt_engineering\n",
        "\n",
        "        # Consistent transform for both sketch and image\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((config.resolution, config.resolution), interpolation=transforms.InterpolationMode.LANCZOS),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
        "        ])\n",
        "\n",
        "        # Furniture-specific prompt collection for variety\n",
        "        self.furniture_prompts = [\n",
        "            \"a detailed, high-quality photograph of furniture\",\n",
        "            \"a professional photograph of home furniture in natural lighting\",\n",
        "            \"a realistic photograph of furniture with clean details\",\n",
        "            \"a high-resolution image of furniture from sketch\",\n",
        "            \"a photorealistic rendering of furniture design\",\n",
        "            \"a detailed furniture photograph with accurate textures and materials\",\n",
        "            \"a professional product photograph of furniture piece\",\n",
        "            \"a clear, detailed image of furniture with realistic details\",\n",
        "        ]\n",
        "\n",
        "        self.default_prompt = \"a detailed, high-quality photograph of furniture generated from sketch\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sketch_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load sketch and target image\n",
        "        sketch_path = self.sketch_paths[idx]\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "            sketch = Image.open(sketch_path).convert(\"RGB\")\n",
        "            target_image = Image.open(image_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading images: {e}\")\n",
        "            print(f\"Sketch path: {sketch_path}\")\n",
        "            print(f\"Image path: {image_path}\")\n",
        "            # Return a default item if there's an error\n",
        "            sketch = Image.new(\"RGB\", (config.resolution, config.resolution), color=\"white\")\n",
        "            target_image = Image.new(\"RGB\", (config.resolution, config.resolution), color=\"white\")\n",
        "\n",
        "        # Apply transformations\n",
        "        sketch_tensor = self.transform(sketch)\n",
        "        target_tensor = self.transform(target_image)\n",
        "\n",
        "        # Select a prompt (either fixed or random from collection)\n",
        "        if self.prompt_engineering:\n",
        "            prompt = np.random.choice(self.furniture_prompts)\n",
        "        else:\n",
        "            prompt = self.default_prompt\n",
        "\n",
        "        # Encode the text prompt\n",
        "        text_inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        text_input_ids = text_inputs.input_ids[0]\n",
        "\n",
        "        return {\n",
        "            \"sketch\": sketch_tensor,\n",
        "            \"target\": target_tensor,\n",
        "            \"input_ids\": text_input_ids,\n",
        "            \"file_path\": image_path\n",
        "        }\n",
        "\n",
        "# Helper function to create directories if they don't exist\n",
        "def create_directories():\n",
        "    os.makedirs(config.output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(config.output_dir, \"samples\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(config.output_dir, \"checkpoints\"), exist_ok=True)"
      ],
      "metadata": {
        "id": "__3Qn6S3X7zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save sample images during training\n",
        "def save_samples(controlnet, unet, vae, text_encoder, tokenizer, noise_scheduler, sketch_batch, epoch, device, show_images=True):\n",
        "    # Create pipeline for inference\n",
        "    pipeline = StableDiffusionControlNetPipeline(\n",
        "        vae=vae,\n",
        "        text_encoder=text_encoder,\n",
        "        tokenizer=tokenizer,\n",
        "        unet=unet,\n",
        "        controlnet=controlnet,\n",
        "        scheduler=noise_scheduler,\n",
        "        safety_checker=None,\n",
        "        feature_extractor=None,\n",
        "        requires_safety_checker=False\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    pipeline = pipeline.to(device)\n",
        "\n",
        "    # Get a few sketch samples\n",
        "    sketches = sketch_batch[\"sketch\"].to(device)\n",
        "    targets = sketch_batch[\"target\"].to(device)\n",
        "    num_samples = min(4, len(sketches))\n",
        "\n",
        "    sketches = sketches[:num_samples]\n",
        "    targets = targets[:num_samples]\n",
        "\n",
        "    prompt = [\"a detailed, high-quality photograph generated from a sketch\"] * num_samples\n",
        "\n",
        "    # Generate images\n",
        "    with torch.no_grad():\n",
        "        images = pipeline(\n",
        "            prompt=prompt,\n",
        "            image=sketches,\n",
        "            num_inference_steps=100,\n",
        "            guidance_scale=10.0\n",
        "        ).images\n",
        "\n",
        "    # Convert tensors and images\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        sketch_img = (sketches[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5) * 255\n",
        "        sketch_img = sketch_img.clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        target_img = (targets[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5) * 255\n",
        "        target_img = target_img.clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        if num_samples > 1:\n",
        "            axes[i, 0].imshow(sketch_img)\n",
        "            axes[i, 0].set_title(\"Sketch Input\")\n",
        "            axes[i, 0].axis(\"off\")\n",
        "\n",
        "            axes[i, 1].imshow(images[i])\n",
        "            axes[i, 1].set_title(\"Generated Image\")\n",
        "            axes[i, 1].axis(\"off\")\n",
        "\n",
        "            axes[i, 2].imshow(target_img)\n",
        "            axes[i, 2].set_title(\"Target Image\")\n",
        "            axes[i, 2].axis(\"off\")\n",
        "        else:\n",
        "            axes[0].imshow(sketch_img)\n",
        "            axes[0].set_title(\"Sketch Input\")\n",
        "            axes[0].axis(\"off\")\n",
        "\n",
        "            axes[1].imshow(images[i])\n",
        "            axes[1].set_title(\"Generated Image\")\n",
        "            axes[1].axis(\"off\")\n",
        "\n",
        "            axes[2].imshow(target_img)\n",
        "            axes[2].set_title(\"Target Image\")\n",
        "            axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config.output_dir, \"samples\", f\"sample_epoch_{epoch}.png\"))\n",
        "\n",
        "    if show_images:\n",
        "        plt.show()  # <-- Show directly in notebook or terminal with GUI support\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "ciG972f-X9qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_controlnet(train_sketch_paths, train_image_paths, val_sketch_paths, val_image_paths):\n",
        "    create_directories()\n",
        "\n",
        "    add_error_handling()\n",
        "\n",
        "    try:\n",
        "        import gc\n",
        "        # Force garbage collection to clean up any existing accelerator\n",
        "        gc.collect()\n",
        "        try:\n",
        "            from accelerate.state import AcceleratorState\n",
        "            if hasattr(AcceleratorState, '_state'):\n",
        "                if AcceleratorState._state is not None:\n",
        "                    AcceleratorState._state = None\n",
        "            elif hasattr(AcceleratorState, 'reset_state'):\n",
        "                AcceleratorState.reset_state()\n",
        "        except (ImportError, AttributeError):\n",
        "            pass\n",
        "\n",
        "        import os\n",
        "        os.environ[\"ACCELERATE_STATE_INITIALIZED\"] = \"0\"\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Could not reset accelerator state: {e}\")\n",
        "        print(\"Proceeding with initialization anyway...\")\n",
        "\n",
        "    # Initialize accelerator with fallback options\n",
        "    try:\n",
        "        accelerator = Accelerator(\n",
        "            mixed_precision=\"no\",\n",
        "            gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "            log_with=\"tensorboard\",\n",
        "            project_dir=os.path.join(config.output_dir, \"logs\")\n",
        "        )\n",
        "\n",
        "        # Initialize logging\n",
        "        accelerator.init_trackers(\"controlnet_training\")\n",
        "    except TypeError as e:\n",
        "        print(f\"Warning: Error initializing accelerator with full config: {e}\")\n",
        "        print(\"Trying with simplified configuration...\")\n",
        "        accelerator = Accelerator(\n",
        "            mixed_precision=\"no\",\n",
        "            gradient_accumulation_steps=config.gradient_accumulation_steps\n",
        "        )\n",
        "        print(\"Accelerator initialized with basic configuration.\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(config.pretrained_model_name, subfolder=\"tokenizer\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = SketchToImageDataset(train_sketch_paths, train_image_paths, tokenizer)\n",
        "    val_dataset = SketchToImageDataset(val_sketch_paths, val_image_paths, tokenizer)\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.val_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Load models\n",
        "    noise_scheduler = DDPMScheduler.from_pretrained(config.pretrained_model_name, subfolder=\"scheduler\")\n",
        "\n",
        "    # Load the VAE component\n",
        "    from diffusers import AutoencoderKL\n",
        "    vae = AutoencoderKL.from_pretrained(config.pretrained_model_name, subfolder=\"vae\")\n",
        "\n",
        "    text_encoder = CLIPTextModel.from_pretrained(config.pretrained_model_name, subfolder=\"text_encoder\")\n",
        "\n",
        "    # Load the UNet\n",
        "    unet = UNet2DConditionModel.from_pretrained(config.pretrained_model_name, subfolder=\"unet\")\n",
        "\n",
        "    # Create a ControlNet model from the UNet\n",
        "    controlnet = ControlNetModel.from_unet(unet)\n",
        "\n",
        "    # Freeze vae and text_encoder\n",
        "    vae.requires_grad_(False)\n",
        "    text_encoder.requires_grad_(False)\n",
        "    unet.requires_grad_(False)  # We only train ControlNet\n",
        "\n",
        "    # Optimizer for ControlNet with weight decay\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        controlnet.parameters(),\n",
        "        lr=config.learning_rate,\n",
        "        weight_decay=1e-2  # Added weight decay to improve generalization\n",
        "    )\n",
        "\n",
        "    # Calculate total number of training steps\n",
        "    total_train_batch_size = config.train_batch_size * accelerator.num_processes * config.gradient_accumulation_steps\n",
        "    num_update_steps_per_epoch = len(train_dataloader) // config.gradient_accumulation_steps\n",
        "    max_train_steps = config.num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=config.lr_scheduler,\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=config.lr_warmup_steps * accelerator.num_processes,\n",
        "        num_training_steps=max_train_steps,\n",
        "    )\n",
        "\n",
        "    # Prepare everything with accelerator with fallback\n",
        "    try:\n",
        "        controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
        "            controlnet, optimizer, train_dataloader, val_dataloader, lr_scheduler\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error in accelerator.prepare: {e}\")\n",
        "        print(\"Using manual device management instead.\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        controlnet.to(device)\n",
        "\n",
        "        # Define a simple wrapper class for backward compatibility\n",
        "        class SimpleAccelerator:\n",
        "            def __init__(self, device):\n",
        "                self.device = device\n",
        "                self.sync_gradients = True\n",
        "                self.num_processes = 1\n",
        "\n",
        "            def backward(self, loss):\n",
        "                loss.backward()\n",
        "\n",
        "            def clip_grad_norm_(self, params, max_norm):\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_norm)\n",
        "\n",
        "            def save_model(self, model, output_dir):\n",
        "                model.save_pretrained(output_dir)\n",
        "\n",
        "            def print(self, *args, **kwargs):\n",
        "                print(*args, **kwargs)\n",
        "\n",
        "            def accumulate(self, model):\n",
        "                class NoOpContextManager:\n",
        "                    def __enter__(self): return None\n",
        "                    def __exit__(self, *args): return None\n",
        "                return NoOpContextManager()\n",
        "\n",
        "            def end_training(self):\n",
        "                pass\n",
        "\n",
        "        if 'accelerator' not in locals() or not hasattr(accelerator, 'backward'):\n",
        "            print(\"Creating simple accelerator replacement\")\n",
        "            accelerator = SimpleAccelerator(device)\n",
        "\n",
        "    # Set up device and precision\n",
        "    weight_dtype = torch.float32\n",
        "\n",
        "    # For mixed precision training, we cast the models to the appropriate precision\n",
        "    if hasattr(accelerator, 'mixed_precision'):\n",
        "        if accelerator.mixed_precision == \"fp16\":\n",
        "            weight_dtype = torch.float16\n",
        "        elif accelerator.mixed_precision == \"bf16\":\n",
        "            weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Determine device - use accelerator's device or fall back to CUDA/CPU\n",
        "    device = accelerator.device if hasattr(accelerator, 'device') else None\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Manually selected device: {device}\")\n",
        "\n",
        "    # Cast all models to the correct dtype and device\n",
        "    text_encoder.to(device, dtype=weight_dtype)\n",
        "    vae.to(device, dtype=weight_dtype)\n",
        "    unet.to(device, dtype=weight_dtype)\n",
        "    controlnet.to(device, dtype=weight_dtype)\n",
        "\n",
        "    # We need to keep vae, unet and text_encoder in eval mode\n",
        "    vae.eval()\n",
        "    text_encoder.eval()\n",
        "    unet.eval()\n",
        "\n",
        "    # Set controlnet to train mode\n",
        "    controlnet.train()\n",
        "\n",
        "    # Keep track of losses\n",
        "    global_step = 0\n",
        "    best_loss = float('inf')\n",
        "    best_model_path = None\n",
        "\n",
        "    for epoch in range(config.num_train_epochs):\n",
        "        controlnet.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(controlnet):\n",
        "                # Move inputs to device\n",
        "                sketch = batch[\"sketch\"].to(accelerator.device, dtype=weight_dtype)\n",
        "                target = batch[\"target\"].to(accelerator.device, dtype=weight_dtype)\n",
        "                input_ids = batch[\"input_ids\"].to(accelerator.device)\n",
        "\n",
        "                # Encode target image to latent space\n",
        "                latents = vae.encode(target).latent_dist.sample()\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise and add to latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Encode text\n",
        "                encoder_hidden_states = text_encoder(input_ids)[0]\n",
        "\n",
        "                # Predict noise with ControlNet conditioning - with flexible output handling\n",
        "                try:\n",
        "                    controlnet_output = controlnet(\n",
        "                        noisy_latents,\n",
        "                        timesteps,\n",
        "                        encoder_hidden_states,\n",
        "                        sketch,\n",
        "                        return_dict=True\n",
        "                    )\n",
        "\n",
        "                    # Check if return_dict worked or if we got a tuple/list\n",
        "                    if hasattr(controlnet_output, 'down_block_res_samples'):\n",
        "                        down_block_res_samples = controlnet_output.down_block_res_samples\n",
        "                        mid_block_res_sample = controlnet_output.mid_block_res_sample\n",
        "                    else:\n",
        "                        # Handle tuple output format for older versions\n",
        "                        print(\"Handling tuple output format from ControlNet\")\n",
        "                        down_block_res_samples = controlnet_output[0]\n",
        "                        mid_block_res_sample = controlnet_output[1]\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in ControlNet forward pass: {e}\")\n",
        "                    print(\"Trying alternative calling convention...\")\n",
        "                    controlnet_output = controlnet(\n",
        "                        noisy_latents,\n",
        "                        timesteps,\n",
        "                        conditioning=sketch,\n",
        "                        encoder_hidden_states=encoder_hidden_states\n",
        "                    )\n",
        "                    # Extract components based on output type\n",
        "                    if isinstance(controlnet_output, tuple):\n",
        "                        down_block_res_samples = controlnet_output[0]\n",
        "                        mid_block_res_sample = controlnet_output[1]\n",
        "                    else:\n",
        "                        down_block_res_samples = controlnet_output.down_block_res_samples\n",
        "                        mid_block_res_sample = controlnet_output.mid_block_res_sample\n",
        "\n",
        "                # Get UNet prediction with ControlNet residuals - with flexible output handling\n",
        "                try:\n",
        "                    unet_output = unet(\n",
        "                        sample=noisy_latents,\n",
        "                        timestep=timesteps,\n",
        "                        encoder_hidden_states=encoder_hidden_states,\n",
        "                        down_block_additional_residuals=down_block_res_samples,\n",
        "                        mid_block_additional_residual=mid_block_res_sample,\n",
        "                        return_dict=True\n",
        "                    )\n",
        "\n",
        "                    # Extract prediction based on output type\n",
        "                    if hasattr(unet_output, 'sample'):\n",
        "                        model_pred = unet_output.sample\n",
        "                    else:\n",
        "                        model_pred = unet_output[0]\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in UNet forward pass: {e}\")\n",
        "                    print(\"Trying alternative calling convention...\")\n",
        "                    unet_output = unet(\n",
        "                        noisy_latents,\n",
        "                        timesteps,\n",
        "                        encoder_hidden_states=encoder_hidden_states\n",
        "                    )\n",
        "                    # Handle output format\n",
        "                    if isinstance(unet_output, tuple):\n",
        "                        model_pred = unet_output[0]\n",
        "                    else:\n",
        "                        model_pred = unet_output.sample\n",
        "\n",
        "                # Compute loss\n",
        "                loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(controlnet.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    lr_scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if global_step % config.validation_steps == 0:\n",
        "                accelerator.print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "            if global_step % 100 == 0:  # Every a100 steps, log the loss\n",
        "                print(f\"Epoch [{epoch + 1}/{config.num_train_epochs}], Step [{global_step}], Loss: {total_loss / (step + 1)}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader)\n",
        "        accelerator.print(f\"Epoch {epoch} completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            best_model_path = os.path.join(config.output_dir, \"checkpoints\", f\"best_model_epoch_{epoch}\")\n",
        "            accelerator.save_model(controlnet, best_model_path)\n",
        "\n",
        "            # Save the entire model with its configuration\n",
        "            controlnet.save_pretrained(best_model_path)  # This will create the config.json\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch} with loss {best_loss:.4f}\")\n",
        "\n",
        "\n",
        "        # Save images and model\n",
        "        if epoch % config.save_images_epochs == 0:\n",
        "            val_batch = next(iter(val_dataloader))\n",
        "            save_samples(controlnet, unet, vae, text_encoder, tokenizer, noise_scheduler, val_batch, epoch, accelerator.device)\n",
        "\n",
        "        if epoch % config.save_model_epochs == 0:\n",
        "            model_path = os.path.join(config.output_dir, \"checkpoints\", f\"controlnet_epoch_{epoch}\")\n",
        "            accelerator.save_model(controlnet, model_path)\n",
        "\n",
        "    accelerator.end_training()\n",
        "\n",
        "    # Return the path to the best model\n",
        "    if best_model_path is None:\n",
        "        best_model_path = os.path.join(config.output_dir, \"checkpoints\", f\"controlnet_epoch_{config.num_train_epochs-1}\")\n",
        "\n",
        "    return best_model_path"
      ],
      "metadata": {
        "id": "sVert7yoX_vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference(model_path, test_sketch_paths, test_image_paths, num_samples=5):\n",
        "    import os\n",
        "    print(f\"Running inference with model from: {model_path}\")\n",
        "\n",
        "    # Load the trained ControlNet model\n",
        "    try:\n",
        "        controlnet = ControlNetModel.from_pretrained(model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ControlNet model: {e}\")\n",
        "        print(\"Attempting to load with a different method...\")\n",
        "        try:\n",
        "            from diffusers import ControlNetModel\n",
        "            controlnet = ControlNetModel.from_pretrained(model_path, local_files_only=True)\n",
        "        except Exception as sub_e:\n",
        "            print(f\"Second loading attempt failed: {sub_e}\")\n",
        "            print(\"Checking directory contents:\")\n",
        "            import os\n",
        "            print(os.listdir(model_path))\n",
        "            raise RuntimeError(f\"Could not load model from {model_path}\")\n",
        "\n",
        "    # Create the pipeline with device detection\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    try:\n",
        "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "            config.pretrained_model_name,\n",
        "            controlnet=controlnet,\n",
        "            safety_checker=None,\n",
        "            requires_safety_checker=False\n",
        "        ).to(device)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating pipeline: {e}\")\n",
        "        print(\"Trying alternative pipeline configuration...\")\n",
        "        # Fallback with explicit component loading\n",
        "        from diffusers import (\n",
        "            AutoencoderKL,\n",
        "            DDIMScheduler,\n",
        "            StableDiffusionControlNetPipeline,\n",
        "            UNet2DConditionModel\n",
        "        )\n",
        "        from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "        # Load individual components\n",
        "        vae = AutoencoderKL.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"vae\"\n",
        "        )\n",
        "        unet = UNet2DConditionModel.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"unet\"\n",
        "        )\n",
        "        tokenizer = CLIPTokenizer.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"tokenizer\"\n",
        "        )\n",
        "        text_encoder = CLIPTextModel.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"text_encoder\"\n",
        "        )\n",
        "        scheduler = DDIMScheduler.from_pretrained(\n",
        "            config.pretrained_model_name, subfolder=\"scheduler\"\n",
        "        )\n",
        "\n",
        "        # Create pipeline with explicit components\n",
        "        pipe = StableDiffusionControlNetPipeline(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            controlnet=controlnet,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=None,\n",
        "            feature_extractor=None,\n",
        "            requires_safety_checker=False\n",
        "        ).to(device)\n",
        "\n",
        "    # Set the pipeline to use deterministic generation\n",
        "    pipe.scheduler = DDPMScheduler.from_pretrained(config.pretrained_model_name, subfolder=\"scheduler\")\n",
        "\n",
        "    # Create a directory for the results\n",
        "    results_dir = \"inference_results\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Randomly sample test sketches\n",
        "    indices = np.random.choice(len(test_sketch_paths), min(num_samples, len(test_sketch_paths)), replace=False)\n",
        "\n",
        "    # Transform for input sketches\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((config.resolution, config.resolution)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Generate images for each sample\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Load sketch and target image\n",
        "        sketch_path = test_sketch_paths[idx]\n",
        "        target_path = test_image_paths[idx]\n",
        "\n",
        "        print(f\"Processing sketch: {sketch_path}\")\n",
        "        print(f\"Target image: {target_path}\")\n",
        "\n",
        "        # Load sketch and target image\n",
        "        sketch = Image.open(sketch_path).convert(\"RGB\")\n",
        "        target = Image.open(target_path).convert(\"RGB\")\n",
        "\n",
        "        # Resize both for visualization\n",
        "        sketch = sketch.resize((config.resolution, config.resolution))\n",
        "        target = target.resize((config.resolution, config.resolution))\n",
        "\n",
        "        # Generate image\n",
        "        prompt = \"a detailed, high-quality photograph generated from a sketch\"\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            image=sketch,\n",
        "            num_inference_steps=75,  # More steps for better quality\n",
        "            guidance_scale=7.5\n",
        "        ).images[0]\n",
        "\n",
        "        # Save individual images for comparison\n",
        "        sketch.save(os.path.join(results_dir, f\"sample_{i}_sketch.png\"))\n",
        "        image.save(os.path.join(results_dir, f\"sample_{i}_generated.png\"))\n",
        "        target.save(os.path.join(results_dir, f\"sample_{i}_target.png\"))\n",
        "\n",
        "        # Save the results\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "        axes[0].imshow(sketch)\n",
        "        axes[0].set_title(\"Input Sketch\")\n",
        "        axes[0].axis(\"off\")\n",
        "\n",
        "        axes[1].imshow(image)\n",
        "        axes[1].set_title(\"Generated Image\")\n",
        "        axes[1].axis(\"off\")\n",
        "\n",
        "        axes[2].imshow(target)\n",
        "        axes[2].set_title(\"Ground Truth\")\n",
        "        axes[2].axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(results_dir, f\"comparison_{i}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved comparison image: comparison_{i}.png\")\n",
        "\n",
        "    # Create a summary image with all comparisons\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5 * num_samples))\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "        sketch = Image.open(os.path.join(results_dir, f\"sample_{i}_sketch.png\"))\n",
        "        generated = Image.open(os.path.join(results_dir, f\"sample_{i}_generated.png\"))\n",
        "        target = Image.open(os.path.join(results_dir, f\"sample_{i}_target.png\"))\n",
        "\n",
        "        if num_samples > 1:\n",
        "            axes[i, 0].imshow(sketch)\n",
        "            axes[i, 0].set_title(f\"Sketch {i+1}\")\n",
        "            axes[i, 0].axis(\"off\")\n",
        "\n",
        "            axes[i, 1].imshow(generated)\n",
        "            axes[i, 1].set_title(f\"Generated {i+1}\")\n",
        "            axes[i, 1].axis(\"off\")\n",
        "\n",
        "            axes[i, 2].imshow(target)\n",
        "            axes[i, 2].set_title(f\"Target {i+1}\")\n",
        "            axes[i, 2].axis(\"off\")\n",
        "        else:\n",
        "            axes[0].imshow(sketch)\n",
        "            axes[0].set_title(\"Sketch\")\n",
        "            axes[0].axis(\"off\")\n",
        "\n",
        "            axes[1].imshow(generated)\n",
        "            axes[1].set_title(\"Generated\")\n",
        "            axes[1].axis(\"off\")\n",
        "\n",
        "            axes[2].imshow(target)\n",
        "            axes[2].set_title(\"Target\")\n",
        "            axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(results_dir, \"all_comparisons.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"All inference results saved to: {results_dir}\")\n",
        "    print(f\"Summary image saved as: {os.path.join(results_dir, 'all_comparisons.png')}\")\n",
        "\n",
        "    return os.path.join(results_dir, \"all_comparisons.png\")"
      ],
      "metadata": {
        "id": "alc7xHKQYCLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_sketch_paths = train_drawn_paths\n",
        "    train_image_paths = train_original_paths\n",
        "    val_sketch_paths = val_drawn_paths\n",
        "    val_image_paths = val_original_paths\n",
        "    test_sketch_paths = test_drawn_paths\n",
        "    test_image_paths = test_original_paths\n",
        "\n",
        "    # Train the model\n",
        "    best_model_path = train_controlnet(train_sketch_paths, train_image_paths, val_sketch_paths, val_image_paths)\n",
        "\n",
        "    # Run inference with the best model (not just the final model)\n",
        "    run_inference(\n",
        "        best_model_path,  # Use the best model instead of final model\n",
        "        test_sketch_paths,\n",
        "        test_image_paths,\n",
        "        num_samples=5\n",
        "    )"
      ],
      "metadata": {
        "id": "qjNt99GIYEju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}